{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531effb6-c317-4c0e-b9e5-c52c0a55642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import math\n",
    "import hashlib\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665cb55-0114-4dd9-9972-f9a34b7ba7b5",
   "metadata": {},
   "source": [
    "# Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db681042-0279-4691-84a6-bea80fbfd646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_funtion(key):\n",
    "    \n",
    "    return  '{0:016b}'.format(key)\n",
    "\n",
    "class Bucket:\n",
    "    \n",
    "    def __init__(self,local_depth,index,empty_spaces,id):\n",
    "        \n",
    "        self.id = id\n",
    "        self.local_depth = local_depth\n",
    "        self.index = index\n",
    "        self.empty_spaces = empty_spaces\n",
    "\n",
    "class Directory:\n",
    "    \n",
    "    def  __init__(self,global_depth,directory_records):\n",
    "        \n",
    "        self.global_depth = global_depth,\n",
    "        self.directory_records = directory_records\n",
    "\n",
    "class DirectoryRecord:\n",
    "    \n",
    "    def __init__(self,bucket, hash_prefix):\n",
    "        \n",
    "        self.hash_prefix = hash_prefix\n",
    "        self.value = bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d07855-6682-44c0-ba2c-dbeee31664a6",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aad45c8-7c64-4035-871f-5b24afec8e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_capacity = 2\n",
    "bucket_number = 3\n",
    "global_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4818defb-e9f3-49ab-97d0-94abe9aed24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of buckets\n",
    "bucket1 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 1)\n",
    "bucket2 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 2)\n",
    "\n",
    "# Initialization of directory\n",
    "directory_records = list()  \n",
    "\n",
    "directory_records.append(DirectoryRecord(hash_prefix = 0, bucket = bucket1))\n",
    "directory_records.append(DirectoryRecord(hash_prefix = 1, bucket = bucket2))\n",
    "\n",
    "directory = Directory(global_depth = 1, directory_records = directory_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68d1b369-6313-487d-a762-e92b189358c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.DirectoryRecord at 0x25e3cdcda30>,\n",
       " <__main__.DirectoryRecord at 0x25e3cdcd8b0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3389f1-00ce-4be4-b605-bef5bbe1324d",
   "metadata": {},
   "source": [
    "# Insertion Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95797ec3-5dc0-49a5-905e-ee7fc90a3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(index):\n",
    "    \n",
    "    global directory\n",
    "    global bucket_number\n",
    "    \n",
    "    t_id = index[0]\n",
    "    hash_key = hash_funtion(int(t_id))\n",
    "    \n",
    "    hash_prefix = int(hash_key[-directory.global_depth[0]:], 2)\n",
    "\n",
    "    bucket = directory.directory_records[hash_prefix].value\n",
    "    bucket.index.append(index)\n",
    "    bucket.empty_spaces = int(bucket.empty_spaces)-1\n",
    "\n",
    "    if(bucket.empty_spaces < 0):\n",
    "\n",
    "        tempopary_memory = bucket.index    \n",
    "        bucket.empty_spaces = bucket_capacity\n",
    "        bucket.index = []\n",
    "\n",
    "        if (directory.global_depth[0] > bucket.local_depth):\n",
    "\n",
    "            # NUMBER OF LINKED BUCKETS\n",
    "            number_of_links = 2**(directory.global_depth[0] - bucket.local_depth)\n",
    "            bucket.local_depth = bucket.local_depth + 1\n",
    "            number_of_modify_links = number_of_links/2 \n",
    "\n",
    "            new_bucket = Bucket(local_depth = bucket.local_depth, index=[], empty_spaces = bucket_capacity, id = bucket_number)\n",
    "\n",
    "            for directory_record in directory.directory_records:\n",
    "\n",
    "                if(directory_record.value == bucket):\n",
    "                    if(number_of_modify_links != 0):\n",
    "                        number_of_modify_links = number_of_modify_links - 1\n",
    "                    else:\n",
    "                        directory_record.value = new_bucket\n",
    "                        bucket_number = bucket_number + 1\n",
    "\n",
    "            for i in range(len(tempopary_memory)):\n",
    "                insert(tempopary_memory[i])\n",
    "                \n",
    "\n",
    "        elif (directory.global_depth[0] == bucket.local_depth):\n",
    "            \n",
    "            new_directory_len = 2 * len(directory.directory_records)\n",
    "            new_directory_records = []\n",
    "\n",
    "            for directory_record_number in range(new_directory_len):\n",
    "                new_directory_records.append(DirectoryRecord(hash_prefix=directory_record_number,bucket=Bucket(local_depth=1,index=[],empty_spaces=bucket_capacity,id=bucket_number)))\n",
    "                bucket_number = bucket_number + 1\n",
    "            \n",
    "            new_directory = Directory(global_depth=directory.global_depth[0]+1,directory_records=new_directory_records)\n",
    "\n",
    "            # REHASING\n",
    "\n",
    "            for directory_record in directory.directory_records:\n",
    "                haskey1 = '0'+hash_funtion(directory_record.hash_prefix)\n",
    "                haskey2 = '1'+hash_funtion(directory_record.hash_prefix)\n",
    "                new_index1 = int(haskey1[-directory.global_depth[0]:],2)\n",
    "                new_index2 = int(haskey2[-directory.global_depth[0]:],2)\n",
    "\n",
    "                new_directory.directory_records[new_index1].value = directory_record.value\n",
    "                new_directory.directory_records[new_index2].value = directory_record.value\n",
    "\n",
    "            directory= new_directory\n",
    "\n",
    "            for i in range(len(tempopary_memory)):\n",
    "\n",
    "                insert(tempopary_memory[i],lock)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8659370-ca36-4b92-aaf9-5af8c1cb3bab",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d98b8d5a-4b5a-4ed4-9ab1-7f42a5f7492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_id = 0\n",
    "t_amount = 100\n",
    "u_name = 'David'\n",
    "\n",
    "insert([t_id, t_amount, u_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0a3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547a479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155d09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25380711",
   "metadata": {},
   "source": [
    "# Task 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0dc256b-4b44-4bc1-834a-1c4d3ca375bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Insert 16\n",
      "  Inserting 16 ('16'→ASCII:103) → index:1\n",
      "  >> Inserted 16 into B1\n",
      "\n",
      "--- Directory (global depth = 1, size = 2) ---\n",
      "  0 → [B0 depth=1] []\n",
      "  1 → [B1 depth=1] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 2: Insert 22\n",
      "  Inserting 22 ('22'→ASCII:100) → index:0\n",
      "  >> Inserted 22 into B0\n",
      "\n",
      "--- Directory (global depth = 1, size = 2) ---\n",
      "  0 → [B0 depth=1] [22]\n",
      "  1 → [B1 depth=1] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 3: Insert 26\n",
      "  Inserting 26 ('26'→ASCII:104) → index:0\n",
      "  >> Inserted 26 into B0\n",
      "\n",
      "--- Directory (global depth = 1, size = 2) ---\n",
      "  0 → [B0 depth=1] [22, 26]\n",
      "  1 → [B1 depth=1] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 4: Insert 20\n",
      "  Inserting 20 ('20'→ASCII:98) → index:0\n",
      "  >> Inserted 20 into B0\n",
      "\n",
      "--- Directory (global depth = 1, size = 2) ---\n",
      "  0 → [B0 depth=1] [22, 26, 20]\n",
      "  1 → [B1 depth=1] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 5: Insert 3\n",
      "  Inserting 3 ('3'→ASCII:51) → index:1\n",
      "  >> Inserted 3 into B1\n",
      "\n",
      "--- Directory (global depth = 1, size = 2) ---\n",
      "  0 → [B0 depth=1] [22, 26, 20]\n",
      "  1 → [B1 depth=1] [16, 3]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 6: Insert 1\n",
      "  Inserting 1 ('1'→ASCII:49) → index:1\n",
      "  >> Inserted 1 into B1\n",
      "\n",
      "--- Directory (global depth = 1, size = 2) ---\n",
      "  0 → [B0 depth=1] [22, 26, 20]\n",
      "  1 → [B1 depth=1] [16, 3, 1]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 7: Insert 12\n",
      "  Inserting 12 ('12'→ASCII:99) → index:1\n",
      "  !! Bucket B1 full. Attempting to split...\n",
      "  >> Directory doubled to size 4, global depth now 2\n",
      "  >> Split B1 into B1 and B2\n",
      "  >> Inserted 12 into B2\n",
      "\n",
      "--- Directory (global depth = 2, size = 4) ---\n",
      "  00 → [B0 depth=1] [22, 26, 20]\n",
      "  01 → [B1 depth=2] [1]\n",
      "  10 → (ref:B0)\n",
      "  11 → [B2 depth=2] [16, 3, 12]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 8: Insert 91\n",
      "  Inserting 91 ('91'→ASCII:106) → index:2\n",
      "  !! Bucket B0 full. Attempting to split...\n",
      "  >> Split B0 into B0 and B3\n",
      "  >> Inserted 91 into B3\n",
      "\n",
      "--- Directory (global depth = 2, size = 4) ---\n",
      "  00 → [B0 depth=2] [22, 26]\n",
      "  01 → [B1 depth=2] [1]\n",
      "  10 → [B3 depth=2] [20, 91]\n",
      "  11 → [B2 depth=2] [16, 3, 12]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 9: Insert 28\n",
      "  Inserting 28 ('28'→ASCII:106) → index:2\n",
      "  >> Inserted 28 into B3\n",
      "\n",
      "--- Directory (global depth = 2, size = 4) ---\n",
      "  00 → [B0 depth=2] [22, 26]\n",
      "  01 → [B1 depth=2] [1]\n",
      "  10 → [B3 depth=2] [20, 91, 28]\n",
      "  11 → [B2 depth=2] [16, 3, 12]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 10: Insert 26\n",
      "  Inserting 26 ('26'→ASCII:104) → index:0\n",
      "  !! Duplicate value 26 ignored.\n",
      "\n",
      "--- Directory (global depth = 2, size = 4) ---\n",
      "  00 → [B0 depth=2] [22, 26]\n",
      "  01 → [B1 depth=2] [1]\n",
      "  10 → [B3 depth=2] [20, 91, 28]\n",
      "  11 → [B2 depth=2] [16, 3, 12]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 11: Insert 47\n",
      "  Inserting 47 ('47'→ASCII:107) → index:3\n",
      "  !! Bucket B2 full. Attempting to split...\n",
      "  >> Directory doubled to size 8, global depth now 3\n",
      "  >> Split B2 into B2 and B4\n",
      "  >> Inserted 47 into B2\n",
      "\n",
      "--- Directory (global depth = 3, size = 8) ---\n",
      "  000 → [B0 depth=2] [22, 26]\n",
      "  001 → [B1 depth=2] [1]\n",
      "  010 → [B3 depth=2] [20, 91, 28]\n",
      "  011 → [B2 depth=3] [3, 12, 47]\n",
      "  100 → (ref:B0)\n",
      "  101 → (ref:B1)\n",
      "  110 → (ref:B3)\n",
      "  111 → [B4 depth=3] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 12: Insert 11\n",
      "  Inserting 11 ('11'→ASCII:98) → index:2\n",
      "  !! Bucket B3 full. Attempting to split...\n",
      "  !! Cannot split B3 - all keys have same prefix at next depth\n",
      "  !! ERROR: Cannot insert 11. Bucket B3 is full and cannot be split.\n",
      "\n",
      "--- Directory (global depth = 3, size = 8) ---\n",
      "  000 → [B0 depth=2] [22, 26]\n",
      "  001 → [B1 depth=2] [1]\n",
      "  010 → [B3 depth=2] [20, 91, 28]\n",
      "  011 → [B2 depth=3] [3, 12, 47]\n",
      "  100 → (ref:B0)\n",
      "  101 → (ref:B1)\n",
      "  110 → (ref:B3)\n",
      "  111 → [B4 depth=3] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 13: Insert 13\n",
      "  Inserting 13 ('13'→ASCII:100) → index:4\n",
      "  >> Inserted 13 into B0\n",
      "\n",
      "--- Directory (global depth = 3, size = 8) ---\n",
      "  000 → [B0 depth=2] [22, 26, 13]\n",
      "  001 → [B1 depth=2] [1]\n",
      "  010 → [B3 depth=2] [20, 91, 28]\n",
      "  011 → [B2 depth=3] [3, 12, 47]\n",
      "  100 → (ref:B0)\n",
      "  101 → (ref:B1)\n",
      "  110 → (ref:B3)\n",
      "  111 → [B4 depth=3] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 14: Insert 19\n",
      "  Inserting 19 ('19'→ASCII:106) → index:2\n",
      "  !! Bucket B3 full. Attempting to split...\n",
      "  !! Cannot split B3 - all keys have same prefix at next depth\n",
      "  !! ERROR: Cannot insert 19. Bucket B3 is full and cannot be split.\n",
      "\n",
      "--- Directory (global depth = 3, size = 8) ---\n",
      "  000 → [B0 depth=2] [22, 26, 13]\n",
      "  001 → [B1 depth=2] [1]\n",
      "  010 → [B3 depth=2] [20, 91, 28]\n",
      "  011 → [B2 depth=3] [3, 12, 47]\n",
      "  100 → (ref:B0)\n",
      "  101 → (ref:B1)\n",
      "  110 → (ref:B3)\n",
      "  111 → [B4 depth=3] [16]\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 15: Insert 38\n",
      "  Inserting 38 ('38'→ASCII:107) → index:3\n",
      "  !! Bucket B2 full. Attempting to split...\n",
      "  >> Directory doubled to size 16, global depth now 4\n",
      "  >> Split B2 into B2 and B5\n",
      "  >> Inserted 38 into B5\n",
      "\n",
      "--- Directory (global depth = 4, size = 16) ---\n",
      "  0000 → [B0 depth=2] [22, 26, 13]\n",
      "  0001 → [B1 depth=2] [1]\n",
      "  0010 → [B3 depth=2] [20, 91, 28]\n",
      "  0011 → [B2 depth=4] [3, 12]\n",
      "  0100 → (ref:B0)\n",
      "  0101 → (ref:B1)\n",
      "  0110 → (ref:B3)\n",
      "  0111 → [B4 depth=3] [16]\n",
      "  1000 → (ref:B0)\n",
      "  1001 → (ref:B1)\n",
      "  1010 → (ref:B3)\n",
      "  1011 → [B5 depth=4] [47, 38]\n",
      "  1100 → (ref:B0)\n",
      "  1101 → (ref:B1)\n",
      "  1110 → (ref:B3)\n",
      "  1111 → (ref:B4)\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 16: Insert 47\n",
      "  Inserting 47 ('47'→ASCII:107) → index:11\n",
      "  !! Duplicate value 47 ignored.\n",
      "\n",
      "--- Directory (global depth = 4, size = 16) ---\n",
      "  0000 → [B0 depth=2] [22, 26, 13]\n",
      "  0001 → [B1 depth=2] [1]\n",
      "  0010 → [B3 depth=2] [20, 91, 28]\n",
      "  0011 → [B2 depth=4] [3, 12]\n",
      "  0100 → (ref:B0)\n",
      "  0101 → (ref:B1)\n",
      "  0110 → (ref:B3)\n",
      "  0111 → [B4 depth=3] [16]\n",
      "  1000 → (ref:B0)\n",
      "  1001 → (ref:B1)\n",
      "  1010 → (ref:B3)\n",
      "  1011 → [B5 depth=4] [47, 38]\n",
      "  1100 → (ref:B0)\n",
      "  1101 → (ref:B1)\n",
      "  1110 → (ref:B3)\n",
      "  1111 → (ref:B4)\n",
      "\n",
      "All Buckets:\n",
      "\n",
      "\n",
      "Step 17: Insert 46\n",
      "  Inserting 46 ('46'→ASCII:106) → index:10\n",
      "  !! Bucket B3 full. Attempting to split...\n",
      "  !! Cannot split B3 - all keys have same prefix at next depth\n",
      "  !! ERROR: Cannot insert 46. Bucket B3 is full and cannot be split.\n",
      "\n",
      "--- Directory (global depth = 4, size = 16) ---\n",
      "  0000 → [B0 depth=2] [22, 26, 13]\n",
      "  0001 → [B1 depth=2] [1]\n",
      "  0010 → [B3 depth=2] [20, 91, 28]\n",
      "  0011 → [B2 depth=4] [3, 12]\n",
      "  0100 → (ref:B0)\n",
      "  0101 → (ref:B1)\n",
      "  0110 → (ref:B3)\n",
      "  0111 → [B4 depth=3] [16]\n",
      "  1000 → (ref:B0)\n",
      "  1001 → (ref:B1)\n",
      "  1010 → (ref:B3)\n",
      "  1011 → [B5 depth=4] [47, 38]\n",
      "  1100 → (ref:B0)\n",
      "  1101 → (ref:B1)\n",
      "  1110 → (ref:B3)\n",
      "  1111 → (ref:B4)\n",
      "\n",
      "All Buckets:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Bucket:\n",
    "    _id_counter = 0\n",
    "\n",
    "    def __init__(self, local_depth):\n",
    "        self.id = Bucket._id_counter\n",
    "        Bucket._id_counter += 1\n",
    "        self.local_depth = local_depth\n",
    "        self.data = []\n",
    "\n",
    "    def is_full(self):\n",
    "        return len(self.data) >= 3  # Bucket size limit is 3\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"[B{self.id} depth={self.local_depth}] {self.data}\"\n",
    "\n",
    "\n",
    "class Directory:\n",
    "    def __init__(self):\n",
    "        self.global_depth = 1\n",
    "        self.size = 2 ** self.global_depth\n",
    "        self.buckets = {}\n",
    "        self.directory = []\n",
    "\n",
    "        # Initialize two buckets\n",
    "        b0 = Bucket(local_depth=1)\n",
    "        b1 = Bucket(local_depth=1)\n",
    "        self.buckets[b0.id] = b0\n",
    "        self.buckets[b1.id] = b1\n",
    "        self.directory = [b0.id, b1.id]\n",
    "\n",
    "    def _calculate_ascii_hash(self, value):\n",
    "        \"Calculate the sum of ASCII values of the string\"\n",
    "        str_val = str(value)\n",
    "        ascii_sum = sum(ord(char) for char in str_val)\n",
    "        return ascii_sum\n",
    "\n",
    "    def get_bucket_index(self, hash_value):\n",
    "        \"\"\"Get directory index\"\"\"\n",
    "        return hash_value & ((1 << self.global_depth) - 1)\n",
    "\n",
    "    def get_bucket(self, h):\n",
    "        bucket_id = self.directory[h]\n",
    "        return self.buckets[bucket_id]\n",
    "\n",
    "    def double_directory(self):\n",
    "        # Double the directory\n",
    "        new_directory = self.directory * 2\n",
    "        self.directory = new_directory\n",
    "        self.global_depth += 1\n",
    "        self.size = len(self.directory)\n",
    "        print(f\"  >> Directory doubled to size {self.size}, global depth now {self.global_depth}\")\n",
    "        return True\n",
    "\n",
    "    def can_split_bucket(self, bucket):\n",
    "        \"Check if bucket can be split by deeper hash\"\n",
    "        if len(bucket.data) < 2:\n",
    "            return False\n",
    "\n",
    "        # Check if values have different prefixes at next depth\n",
    "        new_depth = bucket.local_depth + 1\n",
    "        mask = (1 << new_depth) - 1\n",
    "        prefixes = set()\n",
    "\n",
    "        for val in bucket.data:\n",
    "            h = self._calculate_ascii_hash(val)\n",
    "            prefix = h & mask\n",
    "            prefixes.add(prefix)\n",
    "\n",
    "            # If different prefixes found, split is possible\n",
    "            if len(prefixes) > 1:\n",
    "                return True\n",
    "\n",
    "        return False  # All values share the same prefix\n",
    "\n",
    "    def split_bucket(self, bucket_id):\n",
    "        old_bucket = self.buckets[bucket_id]\n",
    "\n",
    "        # Check if split is possible\n",
    "        if not self.can_split_bucket(old_bucket):\n",
    "            print(f\"  !! Cannot split B{old_bucket.id} - all keys have same prefix at next depth\")\n",
    "            return False\n",
    "\n",
    "        old_depth = old_bucket.local_depth\n",
    "        old_bucket.local_depth += 1\n",
    "\n",
    "        # Double directory if needed\n",
    "        if old_bucket.local_depth > self.global_depth:\n",
    "            self.double_directory()\n",
    "\n",
    "        # Create new bucket\n",
    "        new_bucket = Bucket(local_depth=old_bucket.local_depth)\n",
    "        self.buckets[new_bucket.id] = new_bucket\n",
    "\n",
    "        # Update directory pointers\n",
    "        high_bit = 1 << (old_bucket.local_depth - 1)\n",
    "        for i in range(len(self.directory)):\n",
    "            if self.directory[i] == bucket_id:\n",
    "                # Use high bit to decide bucket\n",
    "                if i & high_bit:\n",
    "                    self.directory[i] = new_bucket.id\n",
    "\n",
    "        # Re-distribute old data\n",
    "        temp = old_bucket.data[:]\n",
    "        old_bucket.data.clear()\n",
    "\n",
    "        for val in temp:\n",
    "            h = self._calculate_ascii_hash(val)\n",
    "            # Use new depth to get bucket\n",
    "            bucket_index = h & ((1 << self.global_depth) - 1)\n",
    "            b = self.get_bucket(bucket_index)\n",
    "            b.data.append(val)\n",
    "\n",
    "        print(f\"  >> Split B{bucket_id} into B{bucket_id} and B{new_bucket.id}\")\n",
    "        return True\n",
    "\n",
    "    def insert(self, value):\n",
    "        # Compute ASCII hash\n",
    "        h = self.get_bucket_index(self._calculate_ascii_hash(value))\n",
    "        bucket = self.get_bucket(h)\n",
    "        str_val = str(value)\n",
    "        ascii_sum = self._calculate_ascii_hash(value)\n",
    "        print(f\"  Inserting {value} ('{str_val}'→ASCII:{ascii_sum}) → index:{h}\")\n",
    "\n",
    "        if value in bucket.data:\n",
    "            print(f\"  !! Duplicate value {value} ignored.\")\n",
    "            return\n",
    "\n",
    "        # If bucket full, try to split\n",
    "        if bucket.is_full():\n",
    "            print(f\"  !! Bucket B{bucket.id} full. Attempting to split...\")\n",
    "            if not self.split_bucket(bucket.id):\n",
    "                print(f\"  !! ERROR: Cannot insert {value}. Bucket B{bucket.id} is full and cannot be split.\")\n",
    "                return\n",
    "\n",
    "            else:\n",
    "                # Get bucket again after split\n",
    "                h = self.get_bucket_index(self._calculate_ascii_hash(value))\n",
    "                bucket = self.get_bucket(h)\n",
    "\n",
    "        # Insert value\n",
    "        bucket.data.append(value)\n",
    "        print(f\"  >> Inserted {value} into B{bucket.id}\")\n",
    "\n",
    "    def __str__(self):\n",
    "        result = f\"\\n--- Directory (global depth = {self.global_depth}, size = {self.size}) ---\\n\"\n",
    "        seen_buckets = set()\n",
    "\n",
    "        # Show directory entries\n",
    "        for i, b_id in enumerate(self.directory):\n",
    "            # Show binary index\n",
    "            binary_index = bin(i)[2:].zfill(self.global_depth)[-self.global_depth:]\n",
    "\n",
    "            if b_id not in seen_buckets:\n",
    "                result += f\"  {binary_index} → {self.buckets[b_id]}\\n\"\n",
    "                seen_buckets.add(b_id)\n",
    "            else:\n",
    "                # Reference to existing bucket\n",
    "                result += f\"  {binary_index} → (ref:B{b_id})\\n\"\n",
    "\n",
    "        # Show all buckets\n",
    "        result += \"\\nAll Buckets:\\n\"\n",
    "        for bucket_id, bucket in self.buckets.items():\n",
    "            if bucket_id not in seen_buckets:\n",
    "                result += f\"  {bucket}\\n\"\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    values = [16, 22, 26, 20, 3, 1, 12, 91, 28, 26, 47, 11, 13, 19, 38, 47, 46]\n",
    "    directory = Directory()\n",
    "\n",
    "    for i, val in enumerate(values):\n",
    "        print(f\"\\nStep {i + 1}: Insert {val}\")\n",
    "        directory.insert(val)\n",
    "        print(directory)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34142e52",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d25c2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Classes and Functions\n",
    "class Bucket:\n",
    "    def __init__(self, local_depth, bucket_id):\n",
    "        self.local_depth = local_depth  # Number of bits used for bucket addressing\n",
    "        self.data = []                  # Values stored in this bucket\n",
    "        self.id = bucket_id             # Unique identifier for the bucket\n",
    "    \n",
    "    def is_full(self, capacity):\n",
    "        return len(self.data) >= capacity  #Check if bucket has reached its capacity\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"B{self.id}(ld={self.local_depth}): {sorted(self.data)}\"\n",
    "\n",
    "class Directory:\n",
    "     #Manages the directory and buckets of the extendible hash table\n",
    "        \n",
    "    def __init__(self, global_depth):\n",
    "        self.global_depth = global_depth          # Number of bits used for directory addressing\n",
    "        self.size = 2 ** global_depth             # Current directory size\n",
    "        # Initialize buckets with unique IDs\n",
    "        self.buckets = [Bucket(global_depth, i) for i in range(self.size)]\n",
    "        self.next_bucket_id = self.size           # Track next available bucket ID\n",
    "    \n",
    "    def get_bucket(self, hash_value):\n",
    "        index = hash_value % self.size  # Determine bucket index\n",
    "        return self.buckets[index]\n",
    "    \n",
    "    def double_directory(self):\n",
    "        print(\" Doubling directory size\")\n",
    "        self.global_depth += 1\n",
    "        new_size = 2 ** self.global_depth\n",
    "        \n",
    "        # Create new directory with doubled size\n",
    "        new_buckets = []\n",
    "        for i in range(new_size):\n",
    "            # Point to existing buckets (copy pointers)\n",
    "            old_index = i % (new_size // 2)\n",
    "            new_buckets.append(self.buckets[old_index])\n",
    "        \n",
    "        self.size = new_size\n",
    "        self.buckets = new_buckets\n",
    "    \n",
    "    def split_bucket(self, bucket_idx):\n",
    "        old_bucket = self.buckets[bucket_idx]\n",
    "        print(f\" Splitting bucket B{old_bucket.id}\")\n",
    "        \n",
    "        # Create new bucket with incremented local depth\n",
    "        new_bucket = Bucket(old_bucket.local_depth + 1, self.next_bucket_id)\n",
    "        self.next_bucket_id += 1\n",
    "        \n",
    "        # Update local depths\n",
    "        old_bucket.local_depth += 1\n",
    "        new_bucket.local_depth = old_bucket.local_depth\n",
    "        \n",
    "        # Update directory pointers using high bit\n",
    "        high_bit_mask = 1 << (old_bucket.local_depth - 1)\n",
    "        for i in range(self.size):\n",
    "            if self.buckets[i] == old_bucket and (i & high_bit_mask):\n",
    "                self.buckets[i] = new_bucket\n",
    "        \n",
    "        return new_bucket\n",
    "\n",
    "    def __str__(self):\n",
    "        result = f\"Global Depth: {self.global_depth}\\nDirectory Size: {self.size}\\n\"\n",
    "        for i, bucket in enumerate(self.buckets):\n",
    "            result += f\"{i} -> {bucket}\\n\"\n",
    "        return result\n",
    "\n",
    "def hash_function(value):\n",
    "    # Convert to string and sum ASCII values of characters\n",
    "    return sum(ord(c) for c in str(value))\n",
    "\n",
    "# Hyper Parameters\n",
    "BUCKET_CAPACITY = 3\n",
    "INITIAL_GLOBAL_DEPTH = 1\n",
    "\n",
    "# Insertion Algorithm\n",
    "def insert(directory, value):\n",
    "    # Compute hash value\n",
    "    hash_value = hash_function(value)\n",
    "    bucket = directory.get_bucket(hash_value)\n",
    "    \n",
    "    print(f\"\\nInserting {value} (ASCII sum={hash_value})\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if value in bucket.data:\n",
    "        print(f\"Duplicate value {value} ignored\")\n",
    "        print(directory)\n",
    "        return\n",
    "    \n",
    "    # If bucket has space, insert directly\n",
    "    if len(bucket.data) < BUCKET_CAPACITY:\n",
    "        bucket.data.append(value)\n",
    "        print(f\"Added to B{bucket.id}\")\n",
    "        print(directory)\n",
    "        return\n",
    "    \n",
    "    print(f\"Bucket B{bucket.id} overflow! Splitting required\")\n",
    "    \n",
    "    # Check if directory needs expansion\n",
    "    if bucket.local_depth == directory.global_depth:\n",
    "        directory.double_directory()\n",
    "        # Recompute bucket after directory expansion\n",
    "        hash_value = hash_function(value)\n",
    "        bucket = directory.get_bucket(hash_value)\n",
    "    \n",
    "    # Split the overflowing bucket\n",
    "    new_bucket = directory.split_bucket(bucket.id)\n",
    "    print(f\"Created new bucket B{new_bucket.id}\")\n",
    "    \n",
    "    # Redistribute data from old bucket\n",
    "    all_data = bucket.data + [value]\n",
    "    bucket.data = []\n",
    "    new_bucket.data = []\n",
    "    \n",
    "    # Reinsert all values\n",
    "    for val in all_data:\n",
    "        val_hash = hash_function(val)\n",
    "        target_bucket = directory.get_bucket(val_hash)\n",
    "        target_bucket.data.append(val)\n",
    "    \n",
    "    print(directory)\n",
    "\n",
    "# Deletion Functionality\n",
    "def delete(directory, value):\n",
    "    hash_value = hash_function(value)\n",
    "    bucket = directory.get_bucket(hash_value)\n",
    "    \n",
    "    print(f\"\\nDeleting {value} (ASCII sum={hash_value})\")\n",
    "    \n",
    "    if value in bucket.data:\n",
    "        bucket.data.remove(value)\n",
    "        print(f\"Removed from B{bucket.id}\")\n",
    "    else:\n",
    "        print(f\"Value {value} not found\")\n",
    "    \n",
    "    print(directory)\n",
    "\n",
    "# Statistics Function\n",
    "def get_bucket_statistics(directory):\n",
    "    \"\"\"Calculate statistics about bucket utilization\"\"\"\n",
    "    stats = {\n",
    "        'total_buckets': len(set(directory.buckets)),\n",
    "        'empty_buckets': 0,\n",
    "        'average_utilization': 0.0,\n",
    "        'min_utilization': BUCKET_CAPACITY,\n",
    "        'max_utilization': 0\n",
    "    }\n",
    "    \n",
    "    total_utilization = 0\n",
    "    unique_buckets = set()\n",
    "    \n",
    "    for bucket in directory.buckets:\n",
    "        # Only count each bucket once\n",
    "        if id(bucket) not in unique_buckets:\n",
    "            unique_buckets.add(id(bucket))\n",
    "            utilization = len(bucket.data) / BUCKET_CAPACITY\n",
    "            total_utilization += utilization\n",
    "            \n",
    "            if len(bucket.data) == 0:\n",
    "                stats['empty_buckets'] += 1\n",
    "            \n",
    "            stats['min_utilization'] = min(stats['min_utilization'], utilization)\n",
    "            stats['max_utilization'] = max(stats['max_utilization'], utilization)\n",
    "    \n",
    "    stats['average_utilization'] = total_utilization / stats['total_buckets'] if stats['total_buckets'] > 0 else 0\n",
    "    return stats\n",
    "\n",
    "# File Loading Function (New Feature)\n",
    "def load_from_file(directory, filename):\n",
    "    print(f\"\\nLoading data from {filename}\")\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                values = line.strip().split(',')\n",
    "                for value in values:\n",
    "                    if value:  # Skip empty strings\n",
    "                        insert(directory, int(value))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filename} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a779443f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EXTENDIBLE HASHING =====\n",
      "\n",
      " CASE 1: Basic Insertion\n",
      "\n",
      "Inserting 16 (ASCII sum=103)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 22 (ASCII sum=100)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [22]\n",
      "1 -> B1(ld=1): [16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 26 (ASCII sum=104)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [22, 26]\n",
      "1 -> B1(ld=1): [16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 20 (ASCII sum=98)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20, 22, 26]\n",
      "1 -> B1(ld=1): [16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 3 (ASCII sum=51)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20, 22, 26]\n",
      "1 -> B1(ld=1): [3, 16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 1 (ASCII sum=49)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20, 22, 26]\n",
      "1 -> B1(ld=1): [1, 3, 16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 12 (ASCII sum=99)\n",
      "Bucket B1 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B1\n",
      "Created new bucket B2\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=1): [20, 22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B0(ld=1): [20, 22, 26]\n",
      "3 -> B2(ld=2): [3, 12, 16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 91 (ASCII sum=106)\n",
      "Bucket B0 overflow! Splitting required\n",
      " Splitting bucket B0\n",
      "Created new bucket B3\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): [22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=2): [20, 91]\n",
      "3 -> B2(ld=2): [3, 12, 16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 28 (ASCII sum=106)\n",
      "Added to B3\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): [22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=2): [20, 28, 91]\n",
      "3 -> B2(ld=2): [3, 12, 16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 26 (ASCII sum=104)\n",
      "Duplicate value 26 ignored\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): [22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=2): [20, 28, 91]\n",
      "3 -> B2(ld=2): [3, 12, 16]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 47 (ASCII sum=107)\n",
      "Bucket B2 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B3\n",
      "Created new bucket B4\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=3): [20, 28, 91]\n",
      "3 -> B2(ld=2): [3, 12, 16, 47]\n",
      "4 -> B0(ld=2): [22, 26]\n",
      "5 -> B1(ld=2): [1]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [3, 12, 16, 47]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 11 (ASCII sum=98)\n",
      "Bucket B3 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B2\n",
      "Created new bucket B5\n",
      "Global Depth: 4\n",
      "Directory Size: 16\n",
      "0 -> B0(ld=2): [22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=3): [11, 20, 28, 91]\n",
      "3 -> B2(ld=3): [3, 12, 16, 47]\n",
      "4 -> B0(ld=2): [22, 26]\n",
      "5 -> B1(ld=2): [1]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B5(ld=3): []\n",
      "8 -> B0(ld=2): [22, 26]\n",
      "9 -> B1(ld=2): [1]\n",
      "10 -> B3(ld=3): [11, 20, 28, 91]\n",
      "11 -> B2(ld=3): [3, 12, 16, 47]\n",
      "12 -> B0(ld=2): [22, 26]\n",
      "13 -> B1(ld=2): [1]\n",
      "14 -> B4(ld=3): []\n",
      "15 -> B5(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 13 (ASCII sum=100)\n",
      "Added to B0\n",
      "Global Depth: 4\n",
      "Directory Size: 16\n",
      "0 -> B0(ld=2): [13, 22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=3): [11, 20, 28, 91]\n",
      "3 -> B2(ld=3): [3, 12, 16, 47]\n",
      "4 -> B0(ld=2): [13, 22, 26]\n",
      "5 -> B1(ld=2): [1]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B5(ld=3): []\n",
      "8 -> B0(ld=2): [13, 22, 26]\n",
      "9 -> B1(ld=2): [1]\n",
      "10 -> B3(ld=3): [11, 20, 28, 91]\n",
      "11 -> B2(ld=3): [3, 12, 16, 47]\n",
      "12 -> B0(ld=2): [13, 22, 26]\n",
      "13 -> B1(ld=2): [1]\n",
      "14 -> B4(ld=3): []\n",
      "15 -> B5(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 19 (ASCII sum=106)\n",
      "Bucket B3 overflow! Splitting required\n",
      " Splitting bucket B2\n",
      "Created new bucket B6\n",
      "Global Depth: 4\n",
      "Directory Size: 16\n",
      "0 -> B0(ld=2): [13, 22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=3): [11, 19, 20, 28, 91]\n",
      "3 -> B2(ld=4): [3, 12, 16, 47]\n",
      "4 -> B0(ld=2): [13, 22, 26]\n",
      "5 -> B1(ld=2): [1]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B5(ld=3): []\n",
      "8 -> B0(ld=2): [13, 22, 26]\n",
      "9 -> B1(ld=2): [1]\n",
      "10 -> B3(ld=3): [11, 19, 20, 28, 91]\n",
      "11 -> B6(ld=4): []\n",
      "12 -> B0(ld=2): [13, 22, 26]\n",
      "13 -> B1(ld=2): [1]\n",
      "14 -> B4(ld=3): []\n",
      "15 -> B5(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 38 (ASCII sum=107)\n",
      "Added to B6\n",
      "Global Depth: 4\n",
      "Directory Size: 16\n",
      "0 -> B0(ld=2): [13, 22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=3): [11, 19, 20, 28, 91]\n",
      "3 -> B2(ld=4): [3, 12, 16, 47]\n",
      "4 -> B0(ld=2): [13, 22, 26]\n",
      "5 -> B1(ld=2): [1]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B5(ld=3): []\n",
      "8 -> B0(ld=2): [13, 22, 26]\n",
      "9 -> B1(ld=2): [1]\n",
      "10 -> B3(ld=3): [11, 19, 20, 28, 91]\n",
      "11 -> B6(ld=4): [38]\n",
      "12 -> B0(ld=2): [13, 22, 26]\n",
      "13 -> B1(ld=2): [1]\n",
      "14 -> B4(ld=3): []\n",
      "15 -> B5(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 47 (ASCII sum=107)\n",
      "Added to B6\n",
      "Global Depth: 4\n",
      "Directory Size: 16\n",
      "0 -> B0(ld=2): [13, 22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=3): [11, 19, 20, 28, 91]\n",
      "3 -> B2(ld=4): [3, 12, 16, 47]\n",
      "4 -> B0(ld=2): [13, 22, 26]\n",
      "5 -> B1(ld=2): [1]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B5(ld=3): []\n",
      "8 -> B0(ld=2): [13, 22, 26]\n",
      "9 -> B1(ld=2): [1]\n",
      "10 -> B3(ld=3): [11, 19, 20, 28, 91]\n",
      "11 -> B6(ld=4): [38, 47]\n",
      "12 -> B0(ld=2): [13, 22, 26]\n",
      "13 -> B1(ld=2): [1]\n",
      "14 -> B4(ld=3): []\n",
      "15 -> B5(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 46 (ASCII sum=106)\n",
      "Bucket B3 overflow! Splitting required\n",
      " Splitting bucket B2\n",
      "Created new bucket B7\n",
      "Global Depth: 4\n",
      "Directory Size: 16\n",
      "0 -> B0(ld=2): [13, 22, 26]\n",
      "1 -> B1(ld=2): [1]\n",
      "2 -> B3(ld=3): [11, 19, 20, 28, 46, 91]\n",
      "3 -> B2(ld=5): [3, 12, 16, 47]\n",
      "4 -> B0(ld=2): [13, 22, 26]\n",
      "5 -> B1(ld=2): [1]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B5(ld=3): []\n",
      "8 -> B0(ld=2): [13, 22, 26]\n",
      "9 -> B1(ld=2): [1]\n",
      "10 -> B3(ld=3): [11, 19, 20, 28, 46, 91]\n",
      "11 -> B6(ld=4): [38, 47]\n",
      "12 -> B0(ld=2): [13, 22, 26]\n",
      "13 -> B1(ld=2): [1]\n",
      "14 -> B4(ld=3): []\n",
      "15 -> B5(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      " CASE 2: Random Data Generation\n",
      "\n",
      "Inserting 74 (ASCII sum=107)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [74]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 92 (ASCII sum=107)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 9 (ASCII sum=57)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [9, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 59 (ASCII sum=110)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [59]\n",
      "1 -> B1(ld=1): [9, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 19 (ASCII sum=106)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [19, 59]\n",
      "1 -> B1(ld=1): [9, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 55 (ASCII sum=106)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [19, 55, 59]\n",
      "1 -> B1(ld=1): [9, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 30 (ASCII sum=99)\n",
      "Bucket B1 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B1\n",
      "Created new bucket B2\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=1): [19, 55, 59]\n",
      "1 -> B1(ld=2): [9]\n",
      "2 -> B0(ld=1): [19, 55, 59]\n",
      "3 -> B2(ld=2): [30, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 91 (ASCII sum=106)\n",
      "Bucket B0 overflow! Splitting required\n",
      " Splitting bucket B0\n",
      "Created new bucket B3\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): []\n",
      "1 -> B1(ld=2): [9]\n",
      "2 -> B3(ld=2): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [30, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 54 (ASCII sum=105)\n",
      "Added to B1\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): []\n",
      "1 -> B1(ld=2): [9, 54]\n",
      "2 -> B3(ld=2): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [30, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 12 (ASCII sum=99)\n",
      "Bucket B2 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B3\n",
      "Created new bucket B4\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): []\n",
      "1 -> B1(ld=2): [9, 54]\n",
      "2 -> B3(ld=3): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [12, 30, 74, 92]\n",
      "4 -> B0(ld=2): []\n",
      "5 -> B1(ld=2): [9, 54]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [12, 30, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 25 (ASCII sum=103)\n",
      "Bucket B2 overflow! Splitting required\n",
      " Splitting bucket B3\n",
      "Created new bucket B5\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): []\n",
      "1 -> B1(ld=2): [9, 54]\n",
      "2 -> B3(ld=4): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [12, 25, 30, 74, 92]\n",
      "4 -> B0(ld=2): []\n",
      "5 -> B1(ld=2): [9, 54]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [12, 25, 30, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 79 (ASCII sum=112)\n",
      "Added to B0\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [79]\n",
      "1 -> B1(ld=2): [9, 54]\n",
      "2 -> B3(ld=4): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [12, 25, 30, 74, 92]\n",
      "4 -> B0(ld=2): [79]\n",
      "5 -> B1(ld=2): [9, 54]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [12, 25, 30, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 56 (ASCII sum=107)\n",
      "Bucket B2 overflow! Splitting required\n",
      " Splitting bucket B3\n",
      "Created new bucket B6\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [79]\n",
      "1 -> B1(ld=2): [9, 54]\n",
      "2 -> B3(ld=5): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [79]\n",
      "5 -> B1(ld=2): [9, 54]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 36 (ASCII sum=105)\n",
      "Added to B1\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [79]\n",
      "1 -> B1(ld=2): [9, 36, 54]\n",
      "2 -> B3(ld=5): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [79]\n",
      "5 -> B1(ld=2): [9, 36, 54]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 63 (ASCII sum=105)\n",
      "Bucket B1 overflow! Splitting required\n",
      " Splitting bucket B1\n",
      "Created new bucket B7\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [79]\n",
      "1 -> B1(ld=3): [9, 36, 54, 63]\n",
      "2 -> B3(ld=5): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [79]\n",
      "5 -> B7(ld=3): []\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 76 (ASCII sum=109)\n",
      "Added to B7\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [79]\n",
      "1 -> B1(ld=3): [9, 36, 54, 63]\n",
      "2 -> B3(ld=5): [19, 55, 59, 91]\n",
      "3 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [79]\n",
      "5 -> B7(ld=3): [76]\n",
      "6 -> B4(ld=3): []\n",
      "7 -> B2(ld=2): [12, 25, 30, 56, 74, 92]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 20 (ASCII sum=98)\n",
      "Bucket B3 overflow! Splitting required\n",
      " Splitting bucket B2\n",
      "Created new bucket B8\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [79]\n",
      "1 -> B1(ld=3): [9, 36, 54, 63]\n",
      "2 -> B3(ld=5): [19, 20, 55, 91]\n",
      "3 -> B2(ld=3): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [79]\n",
      "5 -> B7(ld=3): [76]\n",
      "6 -> B4(ld=3): [59]\n",
      "7 -> B8(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 50 (ASCII sum=101)\n",
      "Added to B7\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [79]\n",
      "1 -> B1(ld=3): [9, 36, 54, 63]\n",
      "2 -> B3(ld=5): [19, 20, 55, 91]\n",
      "3 -> B2(ld=3): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [79]\n",
      "5 -> B7(ld=3): [50, 76]\n",
      "6 -> B4(ld=3): [59]\n",
      "7 -> B8(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 13 (ASCII sum=100)\n",
      "Added to B0\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [13, 79]\n",
      "1 -> B1(ld=3): [9, 36, 54, 63]\n",
      "2 -> B3(ld=5): [19, 20, 55, 91]\n",
      "3 -> B2(ld=3): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [13, 79]\n",
      "5 -> B7(ld=3): [50, 76]\n",
      "6 -> B4(ld=3): [59]\n",
      "7 -> B8(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      "Inserting 88 (ASCII sum=112)\n",
      "Added to B0\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [13, 79, 88]\n",
      "1 -> B1(ld=3): [9, 36, 54, 63]\n",
      "2 -> B3(ld=5): [19, 20, 55, 91]\n",
      "3 -> B2(ld=3): [12, 25, 30, 56, 74, 92]\n",
      "4 -> B0(ld=2): [13, 79, 88]\n",
      "5 -> B7(ld=3): [50, 76]\n",
      "6 -> B4(ld=3): [59]\n",
      "7 -> B8(ld=3): []\n",
      "\n",
      "==================================================\n",
      "\n",
      " CASE 3: Edge Cases\n",
      "\n",
      "Inserting 10 (ASCII sum=97)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [10]\n",
      "\n",
      "\n",
      "Inserting 10 (ASCII sum=97)\n",
      "Duplicate value 10 ignored\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [10]\n",
      "\n",
      "\n",
      "Inserting 999999 (ASCII sum=342)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [999999]\n",
      "1 -> B1(ld=1): [10]\n",
      "\n",
      "\n",
      "Inserting -5 (ASCII sum=98)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [-5, 999999]\n",
      "1 -> B1(ld=1): [10]\n",
      "\n",
      "\n",
      " CASE 4: Load from File\n",
      "\n",
      "Loading data from test_data.txt\n",
      "\n",
      "Inserting 5 (ASCII sum=53)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [5]\n",
      "\n",
      "\n",
      "Inserting 10 (ASCII sum=97)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [5, 10]\n",
      "\n",
      "\n",
      "Inserting 15 (ASCII sum=102)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [15]\n",
      "1 -> B1(ld=1): [5, 10]\n",
      "\n",
      "\n",
      "Inserting 20 (ASCII sum=98)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [15, 20]\n",
      "1 -> B1(ld=1): [5, 10]\n",
      "\n",
      "\n",
      "Inserting 25 (ASCII sum=103)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [15, 20]\n",
      "1 -> B1(ld=1): [5, 10, 25]\n",
      "\n",
      "\n",
      "Inserting 30 (ASCII sum=99)\n",
      "Bucket B1 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B1\n",
      "Created new bucket B2\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=1): [15, 20]\n",
      "1 -> B1(ld=2): [5, 10]\n",
      "2 -> B0(ld=1): [15, 20]\n",
      "3 -> B2(ld=2): [25, 30]\n",
      "\n",
      "\n",
      "Inserting 35 (ASCII sum=104)\n",
      "Added to B0\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=1): [15, 20, 35]\n",
      "1 -> B1(ld=2): [5, 10]\n",
      "2 -> B0(ld=1): [15, 20, 35]\n",
      "3 -> B2(ld=2): [25, 30]\n",
      "\n",
      "\n",
      "Inserting 40 (ASCII sum=100)\n",
      "Bucket B0 overflow! Splitting required\n",
      " Splitting bucket B0\n",
      "Created new bucket B3\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): [35, 40]\n",
      "1 -> B1(ld=2): [5, 10]\n",
      "2 -> B3(ld=2): [15, 20]\n",
      "3 -> B2(ld=2): [25, 30]\n",
      "\n",
      "\n",
      "Inserting 45 (ASCII sum=105)\n",
      "Added to B1\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): [35, 40]\n",
      "1 -> B1(ld=2): [5, 10, 45]\n",
      "2 -> B3(ld=2): [15, 20]\n",
      "3 -> B2(ld=2): [25, 30]\n",
      "\n",
      "\n",
      "Inserting 50 (ASCII sum=101)\n",
      "Bucket B1 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B1\n",
      "Created new bucket B4\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [35, 40]\n",
      "1 -> B1(ld=3): [10, 45]\n",
      "2 -> B3(ld=2): [15, 20]\n",
      "3 -> B2(ld=2): [25, 30]\n",
      "4 -> B0(ld=2): [35, 40]\n",
      "5 -> B4(ld=3): [5, 50]\n",
      "6 -> B3(ld=2): [15, 20]\n",
      "7 -> B2(ld=2): [25, 30]\n",
      "\n",
      "\n",
      " CASE 5: Deletion and Statistics\n",
      "\n",
      "Inserting 10 (ASCII sum=97)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): []\n",
      "1 -> B1(ld=1): [10]\n",
      "\n",
      "\n",
      "Inserting 20 (ASCII sum=98)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20]\n",
      "1 -> B1(ld=1): [10]\n",
      "\n",
      "\n",
      "Inserting 30 (ASCII sum=99)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20]\n",
      "1 -> B1(ld=1): [10, 30]\n",
      "\n",
      "\n",
      "Inserting 40 (ASCII sum=100)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20, 40]\n",
      "1 -> B1(ld=1): [10, 30]\n",
      "\n",
      "\n",
      "Inserting 50 (ASCII sum=101)\n",
      "Added to B1\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20, 40]\n",
      "1 -> B1(ld=1): [10, 30, 50]\n",
      "\n",
      "\n",
      "Inserting 60 (ASCII sum=102)\n",
      "Added to B0\n",
      "Global Depth: 1\n",
      "Directory Size: 2\n",
      "0 -> B0(ld=1): [20, 40, 60]\n",
      "1 -> B1(ld=1): [10, 30, 50]\n",
      "\n",
      "\n",
      "Inserting 70 (ASCII sum=103)\n",
      "Bucket B1 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B1\n",
      "Created new bucket B2\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=1): [20, 40, 60]\n",
      "1 -> B1(ld=2): [10, 50]\n",
      "2 -> B0(ld=1): [20, 40, 60]\n",
      "3 -> B2(ld=2): [30, 70]\n",
      "\n",
      "\n",
      "Inserting 80 (ASCII sum=104)\n",
      "Bucket B0 overflow! Splitting required\n",
      " Splitting bucket B0\n",
      "Created new bucket B3\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): [40, 80]\n",
      "1 -> B1(ld=2): [10, 50]\n",
      "2 -> B3(ld=2): [20, 60]\n",
      "3 -> B2(ld=2): [30, 70]\n",
      "\n",
      "\n",
      "Inserting 90 (ASCII sum=105)\n",
      "Added to B1\n",
      "Global Depth: 2\n",
      "Directory Size: 4\n",
      "0 -> B0(ld=2): [40, 80]\n",
      "1 -> B1(ld=2): [10, 50, 90]\n",
      "2 -> B3(ld=2): [20, 60]\n",
      "3 -> B2(ld=2): [30, 70]\n",
      "\n",
      "\n",
      "Inserting 100 (ASCII sum=145)\n",
      "Bucket B1 overflow! Splitting required\n",
      " Doubling directory size\n",
      " Splitting bucket B1\n",
      "Created new bucket B4\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [40, 80]\n",
      "1 -> B1(ld=3): [10, 90, 100]\n",
      "2 -> B3(ld=2): [20, 60]\n",
      "3 -> B2(ld=2): [30, 70]\n",
      "4 -> B0(ld=2): [40, 80]\n",
      "5 -> B4(ld=3): [50]\n",
      "6 -> B3(ld=2): [20, 60]\n",
      "7 -> B2(ld=2): [30, 70]\n",
      "\n",
      "\n",
      "Statistics before deletion:\n",
      "{'total_buckets': 5, 'empty_buckets': 0, 'average_utilization': 0.6666666666666666, 'min_utilization': 0.3333333333333333, 'max_utilization': 1.0}\n",
      "\n",
      "Deleting 20 (ASCII sum=98)\n",
      "Removed from B3\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [40, 80]\n",
      "1 -> B1(ld=3): [10, 90, 100]\n",
      "2 -> B3(ld=2): [60]\n",
      "3 -> B2(ld=2): [30, 70]\n",
      "4 -> B0(ld=2): [40, 80]\n",
      "5 -> B4(ld=3): [50]\n",
      "6 -> B3(ld=2): [60]\n",
      "7 -> B2(ld=2): [30, 70]\n",
      "\n",
      "\n",
      "Deleting 40 (ASCII sum=100)\n",
      "Removed from B0\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [80]\n",
      "1 -> B1(ld=3): [10, 90, 100]\n",
      "2 -> B3(ld=2): [60]\n",
      "3 -> B2(ld=2): [30, 70]\n",
      "4 -> B0(ld=2): [80]\n",
      "5 -> B4(ld=3): [50]\n",
      "6 -> B3(ld=2): [60]\n",
      "7 -> B2(ld=2): [30, 70]\n",
      "\n",
      "\n",
      "Deleting 99 (ASCII sum=114)\n",
      "Value 99 not found\n",
      "Global Depth: 3\n",
      "Directory Size: 8\n",
      "0 -> B0(ld=2): [80]\n",
      "1 -> B1(ld=3): [10, 90, 100]\n",
      "2 -> B3(ld=2): [60]\n",
      "3 -> B2(ld=2): [30, 70]\n",
      "4 -> B0(ld=2): [80]\n",
      "5 -> B4(ld=3): [50]\n",
      "6 -> B3(ld=2): [60]\n",
      "7 -> B2(ld=2): [30, 70]\n",
      "\n",
      "\n",
      "Statistics after deletion:\n",
      "{'total_buckets': 5, 'empty_buckets': 0, 'average_utilization': 0.5333333333333333, 'min_utilization': 0.3333333333333333, 'max_utilization': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"===== EXTENDIBLE HASHING =====\")\n",
    "    \n",
    "    # Case 1: Basic insertion sequence\n",
    "    print(\"\\n CASE 1: Basic Insertion\")\n",
    "    dir1 = Directory(INITIAL_GLOBAL_DEPTH)\n",
    "    values = [16, 22, 26, 20, 3, 1, 12, 91, 28, 26, 47, 11, 13, 19, 38, 47, 46]\n",
    "    for value in values:\n",
    "        insert(dir1, value)\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Case 2: Random data generation\n",
    "    print(\"\\n CASE 2: Random Data Generation\")\n",
    "    import random\n",
    "    dir2 = Directory(INITIAL_GLOBAL_DEPTH)\n",
    "    for _ in range(20):\n",
    "        value = random.randint(1, 100)\n",
    "        insert(dir2, value)\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Case 3: Edge cases\n",
    "    print(\"\\n CASE 3: Edge Cases\")\n",
    "    dir3 = Directory(INITIAL_GLOBAL_DEPTH)\n",
    "    \n",
    "    # Insert duplicate\n",
    "    insert(dir3, 10)\n",
    "    insert(dir3, 10)  # Duplicate\n",
    "    \n",
    "    # Insert empty (should do nothing)\n",
    "    # insert(dir3, None)  # Uncomment to test\n",
    "    \n",
    "    # Insert very large number\n",
    "    insert(dir3, 999999)\n",
    "    \n",
    "    # Insert negative number\n",
    "    insert(dir3, -5)\n",
    "    \n",
    "    # Case 4: Load from file\n",
    "    print(\"\\n CASE 4: Load from File\")\n",
    "    \n",
    "    # Create test data file\n",
    "    with open('test_data.txt', 'w') as f:\n",
    "        f.write(\"5,10,15,20,25,30,35,40,45,50\")\n",
    "    \n",
    "    dir4 = Directory(INITIAL_GLOBAL_DEPTH)\n",
    "    load_from_file(dir4, 'test_data.txt')\n",
    "    \n",
    "    # Case 5: Deletion and statistics\n",
    "    print(\"\\n CASE 5: Deletion and Statistics\")\n",
    "    dir5 = Directory(INITIAL_GLOBAL_DEPTH)\n",
    "    for value in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "        insert(dir5, value)\n",
    "    \n",
    "    stats_before = get_bucket_statistics(dir5)\n",
    "    print(\"\\nStatistics before deletion:\")\n",
    "    print(stats_before)\n",
    "    \n",
    "    delete(dir5, 20)\n",
    "    delete(dir5, 40)\n",
    "    delete(dir5, 99)  # Not found\n",
    "    \n",
    "    stats_after = get_bucket_statistics(dir5)\n",
    "    print(\"\\nStatistics after deletion:\")\n",
    "    print(stats_after)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13188aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbeaa032",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3aa840b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EXTERNAL MERGE SORT =====\n",
      "Generating 100 pages:\n",
      "Page 00: [147, 262, 549, 799]\n",
      "Page 01: [78, 483, 816, 920]\n",
      "Page 02: [290, 458, 736, 757]\n",
      "Page 03: [175, 335, 397, 654]\n",
      "Page 04: [66, 772, 798, 915]\n",
      "Page 05: [20, 536, 731, 919]\n",
      "Page 06: [4, 114, 218, 786]\n",
      "Page 07: [153, 553, 631, 861]\n",
      "Page 08: [199, 405, 485, 636]\n",
      "Page 09: [119, 130, 220, 751]\n",
      "Page 10: [232, 471, 564, 796]\n",
      "Page 11: [101, 425, 846, 981]\n",
      "Page 12: [11, 66, 907, 977]\n",
      "Page 13: [368, 549, 940, 972]\n",
      "Page 14: [325, 559, 603, 784]\n",
      "Page 15: [30, 84, 378, 738]\n",
      "Page 16: [150, 407, 429, 826]\n",
      "Page 17: [198, 334, 673, 768]\n",
      "Page 18: [164, 262, 696, 725]\n",
      "Page 19: [114, 525, 576, 664]\n",
      "Page 20: [124, 497, 623, 798]\n",
      "Page 21: [142, 276, 302, 605]\n",
      "Page 22: [271, 453, 498, 944]\n",
      "Page 23: [59, 618, 824, 828]\n",
      "Page 24: [390, 880, 903, 988]\n",
      "Page 25: [60, 155, 678, 947]\n",
      "Page 26: [210, 243, 766, 882]\n",
      "Page 27: [95, 466, 553, 927]\n",
      "Page 28: [231, 235, 351, 615]\n",
      "Page 29: [157, 435, 561, 576]\n",
      "Page 30: [286, 828, 851, 911]\n",
      "Page 31: [83, 213, 479, 938]\n",
      "Page 32: [508, 688, 808, 920]\n",
      "Page 33: [633, 644, 809, 963]\n",
      "Page 34: [400, 434, 440, 978]\n",
      "Page 35: [363, 401, 447, 667]\n",
      "Page 36: [77, 318, 347, 576]\n",
      "Page 37: [20, 61, 331, 632]\n",
      "Page 38: [461, 517, 770, 852]\n",
      "Page 39: [166, 393, 477, 631]\n",
      "Page 40: [10, 281, 847, 853]\n",
      "Page 41: [153, 272, 989, 996]\n",
      "Page 42: [12, 43, 178, 442]\n",
      "Page 43: [64, 412, 727, 805]\n",
      "Page 44: [53, 118, 236, 920]\n",
      "Page 45: [440, 471, 605, 853]\n",
      "Page 46: [46, 356, 441, 873]\n",
      "Page 47: [254, 522, 699, 714]\n",
      "Page 48: [332, 773, 828, 962]\n",
      "Page 49: [67, 491, 747, 758]\n",
      "Page 50: [392, 417, 724, 849]\n",
      "Page 51: [518, 558, 939, 994]\n",
      "Page 52: [557, 636, 904, 935]\n",
      "Page 53: [4, 77, 487, 791]\n",
      "Page 54: [333, 593, 763, 850]\n",
      "Page 55: [317, 343, 856, 904]\n",
      "Page 56: [130, 396, 730, 915]\n",
      "Page 57: [151, 459, 643, 795]\n",
      "Page 58: [557, 715, 767, 995]\n",
      "Page 59: [47, 237, 367, 544]\n",
      "Page 60: [66, 368, 410, 413]\n",
      "Page 61: [148, 249, 308, 392]\n",
      "Page 62: [41, 151, 947, 966]\n",
      "Page 63: [364, 460, 473, 672]\n",
      "Page 64: [188, 275, 738, 959]\n",
      "Page 65: [235, 384, 899, 956]\n",
      "Page 66: [29, 421, 471, 492]\n",
      "Page 67: [408, 607, 619, 732]\n",
      "Page 68: [377, 381, 513, 701]\n",
      "Page 69: [342, 399, 524, 880]\n",
      "Page 70: [146, 525, 625, 952]\n",
      "Page 71: [48, 56, 341, 893]\n",
      "Page 72: [191, 631, 775, 826]\n",
      "Page 73: [68, 175, 613, 749]\n",
      "Page 74: [266, 409, 563, 836]\n",
      "Page 75: [144, 639, 667, 672]\n",
      "Page 76: [79, 406, 501, 936]\n",
      "Page 77: [16, 209, 672, 880]\n",
      "Page 78: [461, 546, 717, 986]\n",
      "Page 79: [184, 445, 625, 839]\n",
      "Page 80: [80, 237, 367, 507]\n",
      "Page 81: [243, 391, 732, 968]\n",
      "Page 82: [118, 180, 418, 583]\n",
      "Page 83: [94, 554, 724, 825]\n",
      "Page 84: [182, 588, 743, 816]\n",
      "Page 85: [125, 335, 338, 389]\n",
      "Page 86: [37, 280, 393, 507]\n",
      "Page 87: [431, 745, 770, 923]\n",
      "Page 88: [179, 678, 749, 806]\n",
      "Page 89: [121, 338, 948, 953]\n",
      "Page 90: [558, 581, 867, 987]\n",
      "Page 91: [164, 273, 410, 417]\n",
      "Page 92: [423, 437, 634, 813]\n",
      "Page 93: [385, 404, 702, 908]\n",
      "Page 94: [158, 365, 674, 964]\n",
      "Page 95: [252, 263, 480, 898]\n",
      "Page 96: [416, 439, 672, 797]\n",
      "Page 97: [522, 538, 833, 944]\n",
      "Page 98: [462, 649, 662, 766]\n",
      "Page 99: [33, 222, 863, 887]\n",
      "\n",
      "=== SORT PHASE ===\n",
      "Sorting each page individually\n",
      "Sorting complete\n",
      "\n",
      "=== MERGE PHASE ===\n",
      "\n",
      "MERGE PASS 1: Merging 100 runs with buffer size 3\n",
      "  Merge degree: 2, Runs to process: 100\n",
      "  Total runs: 100, Runs per batch: 2, Batches: 50\n",
      "  Batch 1: Merging 2 runs\n",
      "  Batch 2: Merging 2 runs\n",
      "  Batch 3: Merging 2 runs\n",
      "  Batch 4: Merging 2 runs\n",
      "  Batch 5: Merging 2 runs\n",
      "  Batch 6: Merging 2 runs\n",
      "  Batch 7: Merging 2 runs\n",
      "  Batch 8: Merging 2 runs\n",
      "  Batch 9: Merging 2 runs\n",
      "  Batch 10: Merging 2 runs\n",
      "  Batch 11: Merging 2 runs\n",
      "  Batch 12: Merging 2 runs\n",
      "  Batch 13: Merging 2 runs\n",
      "  Batch 14: Merging 2 runs\n",
      "  Batch 15: Merging 2 runs\n",
      "  Batch 16: Merging 2 runs\n",
      "  Batch 17: Merging 2 runs\n",
      "  Batch 18: Merging 2 runs\n",
      "  Batch 19: Merging 2 runs\n",
      "  Batch 20: Merging 2 runs\n",
      "  Batch 21: Merging 2 runs\n",
      "  Batch 22: Merging 2 runs\n",
      "  Batch 23: Merging 2 runs\n",
      "  Batch 24: Merging 2 runs\n",
      "  Batch 25: Merging 2 runs\n",
      "  Batch 26: Merging 2 runs\n",
      "  Batch 27: Merging 2 runs\n",
      "  Batch 28: Merging 2 runs\n",
      "  Batch 29: Merging 2 runs\n",
      "  Batch 30: Merging 2 runs\n",
      "  Batch 31: Merging 2 runs\n",
      "  Batch 32: Merging 2 runs\n",
      "  Batch 33: Merging 2 runs\n",
      "  Batch 34: Merging 2 runs\n",
      "  Batch 35: Merging 2 runs\n",
      "  Batch 36: Merging 2 runs\n",
      "  Batch 37: Merging 2 runs\n",
      "  Batch 38: Merging 2 runs\n",
      "  Batch 39: Merging 2 runs\n",
      "  Batch 40: Merging 2 runs\n",
      "  Batch 41: Merging 2 runs\n",
      "  Batch 42: Merging 2 runs\n",
      "  Batch 43: Merging 2 runs\n",
      "  Batch 44: Merging 2 runs\n",
      "  Batch 45: Merging 2 runs\n",
      "  Batch 46: Merging 2 runs\n",
      "  Batch 47: Merging 2 runs\n",
      "  Batch 48: Merging 2 runs\n",
      "  Batch 49: Merging 2 runs\n",
      "  Batch 50: Merging 2 runs\n",
      "  Created 50 new runs\n",
      "\n",
      "MERGE PASS 2: Merging 50 runs with buffer size 3\n",
      "  Merge degree: 2, Runs to process: 50\n",
      "  Total runs: 50, Runs per batch: 2, Batches: 25\n",
      "  Batch 1: Merging 2 runs\n",
      "  Batch 2: Merging 2 runs\n",
      "  Batch 3: Merging 2 runs\n",
      "  Batch 4: Merging 2 runs\n",
      "  Batch 5: Merging 2 runs\n",
      "  Batch 6: Merging 2 runs\n",
      "  Batch 7: Merging 2 runs\n",
      "  Batch 8: Merging 2 runs\n",
      "  Batch 9: Merging 2 runs\n",
      "  Batch 10: Merging 2 runs\n",
      "  Batch 11: Merging 2 runs\n",
      "  Batch 12: Merging 2 runs\n",
      "  Batch 13: Merging 2 runs\n",
      "  Batch 14: Merging 2 runs\n",
      "  Batch 15: Merging 2 runs\n",
      "  Batch 16: Merging 2 runs\n",
      "  Batch 17: Merging 2 runs\n",
      "  Batch 18: Merging 2 runs\n",
      "  Batch 19: Merging 2 runs\n",
      "  Batch 20: Merging 2 runs\n",
      "  Batch 21: Merging 2 runs\n",
      "  Batch 22: Merging 2 runs\n",
      "  Batch 23: Merging 2 runs\n",
      "  Batch 24: Merging 2 runs\n",
      "  Batch 25: Merging 2 runs\n",
      "  Created 25 new runs\n",
      "\n",
      "MERGE PASS 3: Merging 25 runs with buffer size 3\n",
      "  Merge degree: 2, Runs to process: 25\n",
      "  Total runs: 25, Runs per batch: 2, Batches: 13\n",
      "  Batch 1: Merging 2 runs\n",
      "  Batch 2: Merging 2 runs\n",
      "  Batch 3: Merging 2 runs\n",
      "  Batch 4: Merging 2 runs\n",
      "  Batch 5: Merging 2 runs\n",
      "  Batch 6: Merging 2 runs\n",
      "  Batch 7: Merging 2 runs\n",
      "  Batch 8: Merging 2 runs\n",
      "  Batch 9: Merging 2 runs\n",
      "  Batch 10: Merging 2 runs\n",
      "  Batch 11: Merging 2 runs\n",
      "  Batch 12: Merging 2 runs\n",
      "  Batch 13: Merging 1 runs\n",
      "  Created 13 new runs\n",
      "\n",
      "MERGE PASS 4: Merging 13 runs with buffer size 3\n",
      "  Merge degree: 2, Runs to process: 13\n",
      "  Total runs: 13, Runs per batch: 2, Batches: 7\n",
      "  Batch 1: Merging 2 runs\n",
      "  Batch 2: Merging 2 runs\n",
      "  Batch 3: Merging 2 runs\n",
      "  Batch 4: Merging 2 runs\n",
      "  Batch 5: Merging 2 runs\n",
      "  Batch 6: Merging 2 runs\n",
      "  Batch 7: Merging 1 runs\n",
      "  Created 7 new runs\n",
      "\n",
      "MERGE PASS 5: Merging 7 runs with buffer size 3\n",
      "  Merge degree: 2, Runs to process: 7\n",
      "  Total runs: 7, Runs per batch: 2, Batches: 4\n",
      "  Batch 1: Merging 2 runs\n",
      "  Batch 2: Merging 2 runs\n",
      "  Batch 3: Merging 2 runs\n",
      "  Batch 4: Merging 1 runs\n",
      "  Created 4 new runs\n",
      "\n",
      "MERGE PASS 6: Merging 4 runs with buffer size 3\n",
      "  Merge degree: 2, Runs to process: 4\n",
      "  Total runs: 4, Runs per batch: 2, Batches: 2\n",
      "  Batch 1: Merging 2 runs\n",
      "  Batch 2: Merging 2 runs\n",
      "  Created 2 new runs\n",
      "\n",
      "MERGE PASS 7: Merging 2 runs with buffer size 3\n",
      "  Merge degree: 2, Runs to process: 2\n",
      "  Total runs: 2, Runs per batch: 2, Batches: 1\n",
      "  Batch 1: Merging 2 runs\n",
      "  Created 1 new runs\n",
      "\n",
      "Merge complete in 7 passes\n",
      "Total I/O operations: 901\n",
      "\n",
      "Final sorted pages:\n",
      "Page 000: [4, 4, 10, 11]\n",
      "Page 001: [12, 16, 20, 20]\n",
      "Page 002: [29, 30, 33, 37]\n",
      "Page 003: [41, 43, 46, 47]\n",
      "Page 004: [48, 53, 56, 59]\n",
      "Page 005: [60, 61, 64, 66]\n",
      "Page 006: [66, 66, 67, 68]\n",
      "Page 007: [77, 77, 78, 79]\n",
      "Page 008: [80, 83, 84, 94]\n",
      "Page 009: [95, 101, 114, 114]\n",
      "Page 010: [118, 118, 119, 121]\n",
      "Page 011: [124, 125, 130, 130]\n",
      "Page 012: [142, 144, 146, 147]\n",
      "Page 013: [148, 150, 151, 151]\n",
      "Page 014: [153, 153, 155, 157]\n",
      "Page 015: [158, 164, 164, 166]\n",
      "Page 016: [175, 175, 178, 179]\n",
      "Page 017: [180, 182, 184, 188]\n",
      "Page 018: [191, 198, 199, 209]\n",
      "Page 019: [210, 213, 218, 220]\n",
      "Page 020: [222, 231, 232, 235]\n",
      "Page 021: [235, 236, 237, 237]\n",
      "Page 022: [243, 243, 249, 252]\n",
      "Page 023: [254, 262, 262, 263]\n",
      "Page 024: [266, 271, 272, 273]\n",
      "Page 025: [275, 276, 280, 281]\n",
      "Page 026: [286, 290, 302, 308]\n",
      "Page 027: [317, 318, 325, 331]\n",
      "Page 028: [332, 333, 334, 335]\n",
      "Page 029: [335, 338, 338, 341]\n",
      "Page 030: [342, 343, 347, 351]\n",
      "Page 031: [356, 363, 364, 365]\n",
      "Page 032: [367, 367, 368, 368]\n",
      "Page 033: [377, 378, 381, 384]\n",
      "Page 034: [385, 389, 390, 391]\n",
      "Page 035: [392, 392, 393, 393]\n",
      "Page 036: [396, 397, 399, 400]\n",
      "Page 037: [401, 404, 405, 406]\n",
      "Page 038: [407, 408, 409, 410]\n",
      "Page 039: [410, 412, 413, 416]\n",
      "Page 040: [417, 417, 418, 421]\n",
      "Page 041: [423, 425, 429, 431]\n",
      "Page 042: [434, 435, 437, 439]\n",
      "Page 043: [440, 440, 441, 442]\n",
      "Page 044: [445, 447, 453, 458]\n",
      "Page 045: [459, 460, 461, 461]\n",
      "Page 046: [462, 466, 471, 471]\n",
      "Page 047: [471, 473, 477, 479]\n",
      "Page 048: [480, 483, 485, 487]\n",
      "Page 049: [491, 492, 497, 498]\n",
      "Page 050: [501, 507, 507, 508]\n",
      "Page 051: [513, 517, 518, 522]\n",
      "Page 052: [522, 524, 525, 525]\n",
      "Page 053: [536, 538, 544, 546]\n",
      "Page 054: [549, 549, 553, 553]\n",
      "Page 055: [554, 557, 557, 558]\n",
      "Page 056: [558, 559, 561, 563]\n",
      "Page 057: [564, 576, 576, 576]\n",
      "Page 058: [581, 583, 588, 593]\n",
      "Page 059: [603, 605, 605, 607]\n",
      "Page 060: [613, 615, 618, 619]\n",
      "Page 061: [623, 625, 625, 631]\n",
      "Page 062: [631, 631, 632, 633]\n",
      "Page 063: [634, 636, 636, 639]\n",
      "Page 064: [643, 644, 649, 654]\n",
      "Page 065: [662, 664, 667, 667]\n",
      "Page 066: [672, 672, 672, 672]\n",
      "Page 067: [673, 674, 678, 678]\n",
      "Page 068: [688, 696, 699, 701]\n",
      "Page 069: [702, 714, 715, 717]\n",
      "Page 070: [724, 724, 725, 727]\n",
      "Page 071: [730, 731, 732, 732]\n",
      "Page 072: [736, 738, 738, 743]\n",
      "Page 073: [745, 747, 749, 749]\n",
      "Page 074: [751, 757, 758, 763]\n",
      "Page 075: [766, 766, 767, 768]\n",
      "Page 076: [770, 770, 772, 773]\n",
      "Page 077: [775, 784, 786, 791]\n",
      "Page 078: [795, 796, 797, 798]\n",
      "Page 079: [798, 799, 805, 806]\n",
      "Page 080: [808, 809, 813, 816]\n",
      "Page 081: [816, 824, 825, 826]\n",
      "Page 082: [826, 828, 828, 828]\n",
      "Page 083: [833, 836, 839, 846]\n",
      "Page 084: [847, 849, 850, 851]\n",
      "Page 085: [852, 853, 853, 856]\n",
      "Page 086: [861, 863, 867, 873]\n",
      "Page 087: [880, 880, 880, 882]\n",
      "Page 088: [887, 893, 898, 899]\n",
      "Page 089: [903, 904, 904, 907]\n",
      "Page 090: [908, 911, 915, 915]\n",
      "Page 091: [919, 920, 920, 920]\n",
      "Page 092: [923, 927, 935, 936]\n",
      "Page 093: [938, 939, 940, 944]\n",
      "Page 094: [944, 947, 947, 948]\n",
      "Page 095: [952, 953, 956, 959]\n",
      "Page 096: [962, 963, 964, 966]\n",
      "Page 097: [968, 972, 977, 978]\n",
      "Page 098: [981, 986, 987, 988]\n",
      "Page 099: [989, 994, 995, 996]\n",
      "\n",
      "Verification: Data is sorted\n",
      "Total pages: 100\n",
      "Total items: 400\n",
      "[4, 4, 10, 11, 12, 16, 20, 20, 29, 30, 33, 37, 41, 43, 46, 47, 48, 53, 56, 59, 60, 61, 64, 66, 66, 66, 67, 68, 77, 77, 78, 79, 80, 83, 84, 94, 95, 101, 114, 114, 118, 118, 119, 121, 124, 125, 130, 130, 142, 144, 146, 147, 148, 150, 151, 151, 153, 153, 155, 157, 158, 164, 164, 166, 175, 175, 178, 179, 180, 182, 184, 188, 191, 198, 199, 209, 210, 213, 218, 220, 222, 231, 232, 235, 235, 236, 237, 237, 243, 243, 249, 252, 254, 262, 262, 263, 266, 271, 272, 273, 275, 276, 280, 281, 286, 290, 302, 308, 317, 318, 325, 331, 332, 333, 334, 335, 335, 338, 338, 341, 342, 343, 347, 351, 356, 363, 364, 365, 367, 367, 368, 368, 377, 378, 381, 384, 385, 389, 390, 391, 392, 392, 393, 393, 396, 397, 399, 400, 401, 404, 405, 406, 407, 408, 409, 410, 410, 412, 413, 416, 417, 417, 418, 421, 423, 425, 429, 431, 434, 435, 437, 439, 440, 440, 441, 442, 445, 447, 453, 458, 459, 460, 461, 461, 462, 466, 471, 471, 471, 473, 477, 479, 480, 483, 485, 487, 491, 492, 497, 498, 501, 507, 507, 508, 513, 517, 518, 522, 522, 524, 525, 525, 536, 538, 544, 546, 549, 549, 553, 553, 554, 557, 557, 558, 558, 559, 561, 563, 564, 576, 576, 576, 581, 583, 588, 593, 603, 605, 605, 607, 613, 615, 618, 619, 623, 625, 625, 631, 631, 631, 632, 633, 634, 636, 636, 639, 643, 644, 649, 654, 662, 664, 667, 667, 672, 672, 672, 672, 673, 674, 678, 678, 688, 696, 699, 701, 702, 714, 715, 717, 724, 724, 725, 727, 730, 731, 732, 732, 736, 738, 738, 743, 745, 747, 749, 749, 751, 757, 758, 763, 766, 766, 767, 768, 770, 770, 772, 773, 775, 784, 786, 791, 795, 796, 797, 798, 798, 799, 805, 806, 808, 809, 813, 816, 816, 824, 825, 826, 826, 828, 828, 828, 833, 836, 839, 846, 847, 849, 850, 851, 852, 853, 853, 856, 861, 863, 867, 873, 880, 880, 880, 882, 887, 893, 898, 899, 903, 904, 904, 907, 908, 911, 915, 915, 919, 920, 920, 920, 923, 927, 935, 936, 938, 939, 940, 944, 944, 947, 947, 948, 952, 953, 956, 959, 962, 963, 964, 966, 968, 972, 977, 978, 981, 986, 987, 988, 989, 994, 995, 996]\n"
     ]
    }
   ],
   "source": [
    "class ExternalMergeSort:\n",
    "    def __init__(self, total_pages=100, page_size=4, buffer_size=3):\n",
    "        self.total_pages = total_pages\n",
    "        self.page_size = page_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.io_count = 0\n",
    "        self.pass_count = 0\n",
    "        self.data = []\n",
    "        self.sorted_pages = []  # Store final sorted pages\n",
    "    \n",
    "    def generate_data(self):\n",
    "        print(f\"Generating {self.total_pages} pages:\")\n",
    "        self.data = [\n",
    "            sorted(random.sample(range(1, 1000), self.page_size)) \n",
    "            for _ in range(self.total_pages)\n",
    "        ]\n",
    "        # Display all pages\n",
    "        for i, page in enumerate(self.data):\n",
    "            print(f\"Page {i:02d}: {page}\")\n",
    "    \n",
    "    def simulate_read(self):\n",
    "        self.io_count += 1\n",
    "    \n",
    "    def simulate_write(self):\n",
    "        self.io_count += 1\n",
    "\n",
    "    def sort_phase(self):\n",
    "        print(\"\\n=== SORT PHASE ===\")\n",
    "        print(\"Sorting each page individually\")\n",
    "        # Pages are already sorted when generated, but in practice sorting might be needed\n",
    "        print(\"Sorting complete\")\n",
    "    \n",
    "    def merge_runs(self, runs):\n",
    "        heap = []\n",
    "        iterators = []\n",
    "        \n",
    "        # Initialize heap\n",
    "        for i, run in enumerate(runs):\n",
    "            # Flatten run if it's nested\n",
    "            flat_run = list(chain.from_iterable(run)) if any(isinstance(item, list) for item in run) else run\n",
    "            iterator = iter(flat_run)\n",
    "            try:\n",
    "                first_item = next(iterator)\n",
    "                heapq.heappush(heap, (first_item, i, iterator))\n",
    "            except StopIteration:\n",
    "                pass\n",
    "            iterators.append(iterator)\n",
    "        \n",
    "        # Merge runs\n",
    "        result = []\n",
    "        current_page = []\n",
    "        while heap:\n",
    "            val, run_idx, iterator = heapq.heappop(heap)\n",
    "            current_page.append(val)\n",
    "            \n",
    "            # Check if page is full\n",
    "            if len(current_page) >= self.page_size:\n",
    "                result.append(current_page)\n",
    "                self.simulate_write()  # Simulate disk write\n",
    "                current_page = []\n",
    "            \n",
    "            # Get next item\n",
    "            try:\n",
    "                next_val = next(iterator)\n",
    "                heapq.heappush(heap, (next_val, run_idx, iterator))\n",
    "            except StopIteration:\n",
    "                pass\n",
    "        \n",
    "        # Add last page if there are remaining items\n",
    "        if current_page:\n",
    "            result.append(current_page)\n",
    "            self.simulate_write()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def merge_phase(self):\n",
    "        print(\"\\n=== MERGE PHASE ===\")\n",
    "        # Initial runs: each page is a separate run\n",
    "        runs = [[item for item in page] for page in self.data]\n",
    "        merge_degree = self.buffer_size - 1  # Merge degree = buffer size - 1 (output page)\n",
    "        \n",
    "        pass_num = 0\n",
    "        while len(runs) > 1:\n",
    "            pass_num += 1\n",
    "            new_runs = []\n",
    "            print(f\"\\nMERGE PASS {pass_num}: Merging {len(runs)} runs with buffer size {self.buffer_size}\")\n",
    "            print(f\"  Merge degree: {merge_degree}, Runs to process: {len(runs)}\")\n",
    "            print(f\"  Total runs: {len(runs)}, Runs per batch: {merge_degree}, Batches: {math.ceil(len(runs)/merge_degree)}\")\n",
    "            \n",
    "            # Process runs in batches\n",
    "            for i in range(0, len(runs), merge_degree):\n",
    "                batch = runs[i:i+merge_degree]\n",
    "                batch_num = i//merge_degree + 1\n",
    "                print(f\"  Batch {batch_num}: Merging {len(batch)} runs\")\n",
    "                \n",
    "                # Simulate read for each run in batch\n",
    "                for run in batch:\n",
    "                    self.simulate_read()\n",
    "                \n",
    "                # Merge batch\n",
    "                merged_run = self.merge_runs(batch)\n",
    "                new_runs.append(merged_run)\n",
    "            \n",
    "            runs = new_runs\n",
    "            print(f\"  Created {len(runs)} new runs\")\n",
    "        \n",
    "        # Final result\n",
    "        if runs:\n",
    "            # Flatten final result\n",
    "            self.sorted_pages = []\n",
    "            for run in runs:\n",
    "                if any(isinstance(item, list) for item in run):\n",
    "                    # Flatten if run contains nested lists\n",
    "                    flat_run = list(chain.from_iterable(run))\n",
    "                else:\n",
    "                    flat_run = run\n",
    "                \n",
    "                # Paginate the flattened run\n",
    "                for i in range(0, len(flat_run), self.page_size):\n",
    "                    page = flat_run[i:i+self.page_size]\n",
    "                    self.sorted_pages.append(page)\n",
    "            print(f\"\\nMerge complete in {pass_num} passes\")\n",
    "        else:\n",
    "            self.sorted_pages = []\n",
    "            print(\"\\nMerge complete (no data)\")\n",
    "        \n",
    "        print(f\"Total I/O operations: {self.io_count}\")\n",
    "        \n",
    "        # Display final sorted result\n",
    "        print(\"\\nFinal sorted pages:\")\n",
    "        for i, page in enumerate(self.sorted_pages):\n",
    "            print(f\"Page {i:03d}: {page}\")\n",
    "        \n",
    "        # Verify sorting result\n",
    "        if self.sorted_pages:\n",
    "            all_items = [item for page in self.sorted_pages for item in page]\n",
    "            is_sorted = all(all_items[i] <= all_items[i+1] for i in range(len(all_items)-1))\n",
    "            \n",
    "            print(f\"\\nVerification: Data is {'sorted' if is_sorted else 'NOT sorted'}\")\n",
    "            print(f\"Total pages: {len(self.sorted_pages)}\")\n",
    "            print(f\"Total items: {len(all_items)}\")\n",
    "        else:\n",
    "            print(\"\\nVerification: No data to verify\")\n",
    "        return self.sorted_pages\n",
    "\n",
    "# Main execution\n",
    "print(\"\\n===== EXTERNAL MERGE SORT =====\")\n",
    "ems = ExternalMergeSort(total_pages=100, page_size=4, buffer_size=3)\n",
    "ems.generate_data()\n",
    "ems.sort_phase()\n",
    "\n",
    "# Get final sorted page list\n",
    "sorted_pages = ems.merge_phase()\n",
    "\n",
    "# Display summary view\n",
    "if sorted_pages:\n",
    "    all_sorted_items = [item for page in sorted_pages for item in page]\n",
    "    print(all_sorted_items[:400])\n",
    "else:\n",
    "    print(\"\\nNo sorted data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350d363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3c473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
